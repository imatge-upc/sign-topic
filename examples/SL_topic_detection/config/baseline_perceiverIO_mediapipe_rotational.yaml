# @package _group_

common:
  fp16: false
  log_interval: 2
  wandb_project: SL_TD

task:
  _name: SL_topic_detection
  feats_type: ???
  body_parts: upperbody,lefthand,righthand
  feat_dims: "0,1,2"
  modeling_task: classification
  num_labels: 10
  data: ???
  dict_path: ???
  normalize: true
  min_source_positions: 150
  max_source_positions: 5500
  max_target_positions: 1

dataset:
  num_workers: 0
  max_tokens: 20000  # batch_size is inferred from max_tokens as the max num of sequences that can be fit in max_tokens
  skip_invalid_size_inputs_valid_test: true
  train_subset: train
  valid_subset: val
  validate_interval_updates: 600

criterion:
  _name: label_smoothed_cross_entropy

optimization:
  max_update: 9000
  lr: [0.0001]
  update_freq: [1]

optimizer:
  _name: adam
  adam_betas: [0.9, 0.998]
  weight_decay: 0.001

lr_scheduler:
  _name: reduce_lr_on_plateau
  lr_shrink: 0.5
  lr_patience: 8
  warmup_updates: 500

checkpoint:
  keep_last_epochs: 1
  best_checkpoint_metric: acc
  maximize_best_checkpoint_metric: true
  save_dir: ???

model:
  _name: SL_topic_detection_PerceiverIO
  dropout: 0.1
  d_model: ???
  num_blocks: 1
  num_self_attends_per_block: 1
  num_self_attention_heads: 8
  num_cross_attention_heads: 8
  num_latents: 512  # has to be a multiple of chunk_size_feed_forward
  d_latents: 768
  qk_channels: 256
  decoder_qk_channels: 256
  decoder_v_channels: 256
  cross_attention_shape_for_attention: q
  self_attention_widening_factor: 1
  cross_attention_widening_factor: 1
  hidden_act: relu
  attention_probs_dropout_prob: 0.1
  use_query_residual: true
  preprocessor_position_encoding_type: trainable
  decoder_position_encoding_type: trainable

bpe:
  _name: sentencepiece
  sentencepiece_model: ???
